# Install dependencies
!pip install -q catboost xgboost lightgbm imbalanced-learn optuna joblib numpy==1.24.4 --force-reinstall
!pip install -q kagglehub

# Imports
import kagglehub
import pandas as pd
import numpy as np
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, accuracy_score
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE
import joblib

# Load dataset
path = kagglehub.dataset_download("saurabhbadole/bank-customer-churn-prediction-dataset")
df = pd.read_csv(f"{path}/Churn_Modelling.csv")

# Drop unused columns
df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)

# Encode categorical variables
le = LabelEncoder()
df['Geography'] = le.fit_transform(df['Geography'])
df['Gender'] = le.fit_transform(df['Gender'])

# Feature engineering
df['BalanceSalaryRatio'] = df['Balance'] / (df['EstimatedSalary'] + 1)
df['TenureByAge'] = df['Tenure'] / (df['Age'] + 1)
df['CreditScorePerAge'] = df['CreditScore'] / (df['Age'] + 1)

# Features and labels
X = df.drop('Exited', axis=1)
y = df['Exited']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Apply SMOTE to balance the training data
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_sm)
X_test_scaled = scaler.transform(X_test)

# Optuna hyperparameter tuning for XGBoost
def objective(trial):
    params = {
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "n_estimators": trial.suggest_int("n_estimators", 100, 500),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0)
    }

    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **params)
    model.fit(X_train_scaled, y_train_sm)
    preds = model.predict(X_test_scaled)
    return accuracy_score(y_test, preds)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)
best_params = study.best_params

# Train final models
xgb_final = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **best_params)
cat_final = CatBoostClassifier(verbose=0, iterations=300, depth=6, learning_rate=0.05)
lgbm_final = LGBMClassifier(n_estimators=300, learning_rate=0.05, max_depth=6)
logreg_final = LogisticRegression(max_iter=1000)

xgb_final.fit(X_train_scaled, y_train_sm)
cat_final.fit(X_train_scaled, y_train_sm)
lgbm_final.fit(X_train_scaled, y_train_sm)
logreg_final.fit(X_train_scaled, y_train_sm)

# Create voting ensemble
ensemble = VotingClassifier(
    estimators=[
        ('xgb', xgb_final),
        ('cat', cat_final),
        ('lgbm', lgbm_final),
        ('lr', logreg_final)
    ],
    voting='soft'
)

ensemble.fit(X_train_scaled, y_train_sm)

# Predict on test data
y_pred = ensemble.predict(X_test_scaled)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"\nâœ… Model Accuracy: {accuracy:.4f}\n")
print(classification_report(y_test, y_pred, digits=4))

# Save model and scaler
joblib.dump(ensemble, 'churn_model.pkl')
joblib.dump(scaler, 'scaler.pkl')